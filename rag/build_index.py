
import os
import json
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer

# è¨­å®šè³‡æ–™èˆ‡å„²å­˜è·¯å¾‘
CHUNKING_TYPES = ["statistical", "consecutive", "cumulative"]
VECTOR_BASE = "./vector_base"
INDEX_BASE = "./faiss_index"
MODEL_NAME = "all-MiniLM-L6-v2"

# å»ºç«‹å„²å­˜è³‡æ–™å¤¾
os.makedirs(INDEX_BASE, exist_ok=True)

# è¼‰å…¥ embedding æ¨¡å‹
model = SentenceTransformer(MODEL_NAME)

def build_single_index(chunk_file: str, index_file: str):
    # è®€å– chunks
    with open(chunk_file, "r", encoding="utf-8") as f:
        chunks = json.load(f)

    # ç”¢ç”Ÿå‘é‡
    embeddings = model.encode(chunks, convert_to_numpy=True, show_progress_bar=True)

    # å»ºç«‹ä¸¦å„²å­˜ FAISS Index
    index = faiss.IndexFlatL2(embeddings.shape[1])
    index.add(embeddings)
    faiss.write_index(index, index_file)

    # å„²å­˜ chunks çš„åŸæ–‡ï¼Œä¾›æŸ¥è©¢æ™‚ä½¿ç”¨
    with open(index_file + ".txt", "w", encoding="utf-8") as f:
        for chunk in chunks:
            f.write(chunk + "\n")

    print(f"âœ… å·²å»ºç«‹ä¸¦å„²å­˜ FAISS indexï¼š{index_file}")

if __name__ == "__main__":
    for chunk_type in CHUNKING_TYPES:
        json_path = os.path.join(VECTOR_BASE, f"chunked_{chunk_type}.json")
        index_path = os.path.join(INDEX_BASE, f"{chunk_type}.index")
        build_single_index(json_path, index_path)
    
    print("ğŸ‰ æ‰€æœ‰ chunking é¡å‹çš„ FAISS index å»ºç«‹å®Œæˆï¼")
